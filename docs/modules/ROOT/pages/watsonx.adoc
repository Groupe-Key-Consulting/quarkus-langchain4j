= IBM watsonx.ai

include::./includes/attributes.adoc[]

You can develop generative AI solutions with foundation models in IBM watsonx.ai. You can use prompts to generate, classify, summarize, or extract content from your input text. Choose from IBM models or open source models from Hugging Face. You can tune foundation models to customize your prompt output or optimize inferencing performance.

IMPORTANT: Supported only for IBM watsonx as a service on link:https://www.ibm.com/products/watsonx-ai/foundation-models[IBM Cloud].

== Using watsonx.ai

To employ watsonx.ai LLMs, integrate the following dependency into your project:

[source,xml,subs=attributes+]
----
<dependency>
    <groupId>io.quarkiverse.langchain4j</groupId>
    <artifactId>quarkus-langchain4j-watsonx</artifactId>
    <version>{project-version}</version>
</dependency>
----

If no other extension is installed, xref:ai-services.adoc[AI Services] will automatically utilize the configured watsonx dependency.

=== Configuration
To use the watsonx.ai dependency, you must configure some required values in the `application.properties` file.

==== Base URL
The `base-url` property depends on the region of the provided service instance, use one of the following values:

* Dallas: https://us-south.ml.cloud.ibm.com
* Frankfurt: https://eu-de.ml.cloud.ibm.com
* Tokyo: https://jp-tok.ml.cloud.ibm.com
* London: https://eu-gb.ml.cloud.ibm.com

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.base-url=https://us-south.ml.cloud.ibm.com
----

==== Project ID
To prompt foundation models in watsonx.ai programmatically, you need to pass the identifier (ID) of a project.

To get the ID of a project, complete the following steps:

1. Open the project, and then click the Manage tab.
2. Copy the project ID from the Details section of the General page.

NOTE: To view the list of projects, go to https://dataplatform.cloud.ibm.com/projects/?context=wx.

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.project-id=23d...
----

==== API Key
To prompt foundation models in IBM watsonx.ai programmatically, you need an IBM Cloud API key.

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.api-key=hG-...
----

NOTE: To determine the API key, go to https://cloud.ibm.com/iam/apikeys and generate it.

==== Interacting with Models

The `watsonx.ai` module provides two different modes for interacting with LLM models: `generation` and `chat`. These modes allow you to tailor the interaction based on the complexity of your use case and how much control you want to have over the prompt structure.

You can select the interaction mode using the property `quarkus.langchain4j.watsonx.chat-model.mode`.

* `generation`: In this mode, you must explicitly structure the prompts using the required model-specific tags. This provides full control over the format of the prompt, but requires in-depth knowledge of the model being used. For best results, always refer to the documentation provided of each model to maximize the effectiveness of your prompts.
* `chat`: This mode abstracts the complexity of tagging by automatically formatting prompts so you can focus on the content (*default value*).

To choose between one of these two modes, add the `chat-model.mode` property to your `application.properties` file:

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.mode=chat  // or 'generate'
----

NOTE: Depending on the mode selected, the values for configuring the model are found under the `chat-model` or `generation-model` properties.

==== Chat Mode

In `chat` mode, you can interact with models without having to manually manage the tags of a prompt.

You might choose this mode if you are looking for dynamic interactions where the model can build on previous messages and provide more contextually relevant responses. This mode simplifies the interaction by automatically managing the necessary tags, allowing you to focus on the content of your prompts rather than formatting.

Chat mode also supports the use of `tools`, allowing the model to perform specific actions or retrieve external data as part of its responses. This extends the capabilities of the model, allowing it to perform complex tasks dynamically and adapt to your needs. More information about tools is available on the xref:./agent-and-tools.adoc[Agent and Tools] page.

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.base-url=${BASE_URL}
quarkus.langchain4j.watsonx.api-key=${API_KEY}
quarkus.langchain4j.watsonx.project-id=${PROJECT_ID}
quarkus.langchain4j.watsonx.chat-model.model-id=mistralai/mistral-large
----

[source,java]
----
@RegisterAiService
public interface AiService {
    @SystemMessage("You are a helpful assistant")
    public String chat(@MemoryId String id, @UserMessage message);
}
----

NOTE: The availability of `chat` and `tools` is currently limited to certain models. Not all models support these features, so be sure to consult the documentation for the specific model you are using to confirm whether these features are available.

==== Generation Mode

In `generation` mode, you have complete control over the structure of your prompts by manually specifying tags for a specific model. This mode could be useful in scenarios where a single-response is desired.

[source,properties,subs=attributes+]
----
quarkus.langchain4j.watsonx.base-url=${BASE_URL}
quarkus.langchain4j.watsonx.api-key=${API_KEY}
quarkus.langchain4j.watsonx.project-id=${PROJECT_ID}
quarkus.langchain4j.watsonx.generation-model.model-id=mistralai/mistral-large
quarkus.langchain4j.watsonx.mode=generation
----

[source,java]
----
@RegisterAiService(chatMemoryProviderSupplier = RegisterAiService.NoChatMemoryProviderSupplier.class)
public interface AiService {
    @UserMessage("""
        <s>[INST] You are a helpful assistant [/INST]</s>\
        [INST] What is the capital of {capital}? [/INST]""")
    public String askCapital(String capital);
}
----

NOTE: The `@SystemMessage` and `@UserMessage` annotations are joined by default with a new line. If you want to change this behavior, use the property `quarkus.langchain4j.watsonx.chat-model.prompt-joiner=<value>`. By adjusting this property, you can define your preferred way of joining messages and ensure that the prompt structure meets your specific needs.

NOTE: Sometimes it may be useful to use the `quarkus.langchain4j.watsonx.chat-model.stop-sequences` property to prevent the LLM model from returning more results than desired.

==== All configuration properties

include::includes/quarkus-langchain4j-watsonx.adoc[leveloffset=+1,opts=optional]
