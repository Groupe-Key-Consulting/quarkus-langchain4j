= Ollama

include::./includes/attributes.adoc[]

https://ollama.com/[Ollama] provides a way to run large language models (LLMs) locally.
You can run many https://ollama.com/library[models] such as LLama3, Mistral, CodeLlama and many others on your machine, with full CPU and GPU support.

[#_prerequisites]
== Prerequisites

To use Ollama, you need to have a running Ollama installed.
Ollama is available for all major platforms and its installation is quite easy, simply visit https://ollama.com/download[Ollama download page] and follow the instructions.

Once installed, check that Ollama is running using:

[source,shell]
----
> ollama --version
----

=== Dev Service

Quarkus LangChain4j automatically handles the pulling of the models configured by the application, so there is no need for users to do so manually.

WARNING: Models are huge. For example Llama3 is 4.7Gb, so make sure you have enough disk space.

NOTE: Due to model's large size, pulling them can take time

== Using Ollama

To integrate with models running on Ollama, add the following dependency into your project:

[source,xml,subs=attributes+]
----
<dependency>
    <groupId>io.quarkiverse.langchain4j</groupId>
    <artifactId>quarkus-langchain4j-ollama</artifactId>
    <version>{project-version}</version>
</dependency>
----

If no other LLM extension is installed, link:../ai-services.adoc[AI Services] will automatically utilize the configured Ollama model.

By default, the extension uses `llama3.1`, the model we pulled in the previous section.
You can change it by setting the `quarkus.langchain4j.ollama.chat-model.model-id` property in the `application.properties` file:

[source,properties,subs=attributes+]
----
quarkus.langchain4j.ollama.chat-model.model-id=mistral
----

=== Configuration

Several configuration properties are available:

include::includes/quarkus-langchain4j-ollama.adoc[leveloffset=+1,opts=optional]

== Document Retriever and Embedding

Ollama also provides embedding models.
By default, it uses `nomic-embed-text`.

You can change the default embedding model by setting the `quarkus.langchain4j.ollama.embedding-model.model-id` property in the `application.properties` file:

[source,properties,subs=attributes+]
----
quarkus.langchain4j.log-requests=true
quarkus.langchain4j.log-responses=true

quarkus.langchain4j.ollama.chat-model.model-id=mistral
quarkus.langchain4j.ollama.embedding-model.model-id=mistral
----

If no other LLM extension is installed, retrieve the embedding model as follows:

[source, java]
----
@Inject EmbeddingModel model; // Injects the embedding model
----

== Dynamic Authorization Headers

There are cases where one may need to provide dynamic authorization headers, to be passed to Ollama endpoints

There are two ways to achieve this:

=== Using a ContainerRequestFilter annotated with `@Provider`.
As the underlying HTTP communication relies on the Quarkus Rest Client, it is possible to apply a filter that will be called in all OpenAI requests and set the headers accordingly.

[source,java]
----
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import jakarta.ws.rs.ext.Provider;
import org.jboss.resteasy.reactive.client.spi.ResteasyReactiveClientRequestContext;
import org.jboss.resteasy.reactive.client.spi.ResteasyReactiveClientRequestFilter;

@Provider
@ApplicationScoped
public class RequestFilter implements ResteasyReactiveClientRequestFilter {

    @Inject
    MyAuthorizationService myAuthorizationService;

    @Override
    public void filter(ResteasyReactiveClientRequestContext requestContext) {
        /*
         * All requests will be filtered here, therefore make sure that you make
         * the necessary checks to avoid putting the Authorization header in
         * requests that do not need it.
         */
        requestContext.getHeaders().putSingle("Authorization", ...);
    }
}
----

=== Using `AuthProvider`
An even simpler approach consists of implementing the `ModelAuthProvider` interface and provide the implementation of the `getAuthorization` method.

This is useful when you need to provide different authorization headers for different OpenAI models. The `@ModelName` annotation can be used to specify the model name in this scenario.

[source,java]
----
import io.quarkiverse.langchain4j.ModelName;
import io.quarkiverse.langchain4j.auth.ModelAuthProvider;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;

@ApplicationScoped
@ModelName("my-model-name") //you can omit this if you have only one model or if you want to use the default model
public class TestClass implements ModelAuthProvider {
    @Inject MyTokenProviderService tokenProviderService;

    @Override
    public String getAuthorization(Input input) {
        /*
         * The `input` will contain some information about the request
         * about to be passed to the remote model endpoints
         */
        return "Bearer " + tokenProviderService.getToken();
    }
}
----


